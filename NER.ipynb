{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_B6Mud_sxp2"
      },
      "source": [
        "from nltk.tag import StanfordNERTagger\n",
        "# from nltk.tag import corenlp\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "import pandas as pd"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmdG8WCxvqz9",
        "outputId": "ba7c7af4-afa3-4e2e-89ef-1bbb7c19291f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7G0_nf-8eXP"
      },
      "source": [
        "### Download and import the Stanford NER 4 classes model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEOzGxKSs3gX",
        "outputId": "ac0890d9-9cdd-48cc-aeb5-d4141b97dca3"
      },
      "source": [
        "st = StanfordNERTagger('/content/drive/MyDrive/Colab Notebooks/Capstone_govt_of_canada/StanfordNER/english.conll.4class.distsim.crf.ser.gz',\n",
        "\t\t\t\t\t   '/content/drive/MyDrive/Colab Notebooks/Capstone_govt_of_canada/StanfordNER/stanford-ner-4.2.0.jar',\n",
        "\t\t\t\t\t   encoding='utf-8')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/tag/stanford.py:183: DeprecationWarning: \n",
            "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
            "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
            "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXR05ESb2-7e"
      },
      "source": [
        "NP_POS = {\"DT\", \"NN\", \"JJ\", \"PR\"}  # these are the first two letters of the POS that you should consider potential parts of nouns \n",
        "NP_HEAD_POS = {\"NN\", \"PR\"}  # each chunk must have at least one of these\n",
        "\n",
        "def get_chunks(sentence):\n",
        "    '''Extracts noun phrases from a sentence corresponding to the part-of-speech tags in optional_POS,\n",
        "    requiring at least one of the POS tags in required_POS. Returns the chunks as a list of strings'''\n",
        "\n",
        "    chunks = []\n",
        "    tagged = pos_tag(word_tokenize(sentence))\n",
        "    start = -1\n",
        "    seen_required = False\n",
        "    for i in range(len(tagged)):\n",
        "        if tagged[i][1][:2] in NP_POS:\n",
        "            if start == -1:\n",
        "                start = i\n",
        "            if tagged[i][1][:2] in NP_HEAD_POS:\n",
        "                seen_required = True\n",
        "        else:\n",
        "            if start != -1:\n",
        "                if seen_required:\n",
        "                    chunks.append(\" \".join([pair[0] for pair in tagged[start:i]]))\n",
        "                start = -1\n",
        "                seen_required = False\n",
        "    if start != -1:\n",
        "        if seen_required:\n",
        "            chunks.append(\" \".join([pair[0] for pair in tagged[start:]]))\n",
        "\n",
        "    return chunks"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byC0kIRw-f4C"
      },
      "source": [
        "NER_CLASSES = {\"ORGANIZATION\", \"PERSON\", \"LOCATION\", \"MISC\"}\n",
        "def get_ner_chunks(text):\n",
        "  '''Extracts name entities from a sentence corresponding to the NER tags in Stanford 4 classies,\n",
        "  return the chunks as a list of tuples'''\n",
        "  chunks = []\n",
        "  tokenized_text = word_tokenize(text)\n",
        "  classified_text = st.tag(tokenized_text)\n",
        "  i = 0\n",
        "  # print(classified_text)\n",
        "  while i < len(classified_text):\n",
        "    # print(i)\n",
        "    if classified_text[i][1] in NER_CLASSES:\n",
        "      chunk = []\n",
        "      # print(classified_text[i][1])\n",
        "      for j in range(i, len(classified_text)):\n",
        "        if classified_text[i][1] == classified_text[j][1]:\n",
        "          chunk.append(classified_text[j][0])\n",
        "          # print(chunk)\n",
        "        else:\n",
        "          chunks.append((\" \".join(chunk), classified_text[i][1]))\n",
        "          i=j\n",
        "          break\n",
        "    else:\n",
        "      i+=1\n",
        "  return chunks"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaYnY5iw6F8u"
      },
      "source": [
        "data_path = \"/content/drive/MyDrive/Colab Notebooks/Capstone_govt_of_canada/data/Page feedback-Vaccine pages-May17.csv\"\n",
        "df = pd.read_csv(data_path, encoding=\"utf-8\")\n",
        "df = df[df['Tags confirmed']=='checked'][df['Lang'] == 'EN']\n",
        "train_df, dev_df = train_test_split(df,  test_size=0.2, random_state=11)\n",
        "# train_df"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjm-OZ_x6cp0"
      },
      "source": [
        "text_lst = train_df.Comment.values\n",
        "chunks_lst = []\n",
        "ner_lst = []\n",
        "for text in text_lst:\n",
        "  chunks_lst.append(\" \".join(get_chunks(text)))\n",
        "  # ner_lst.append(get_ner_chunks(text))\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RIEj4ne7Fr6",
        "outputId": "44a1705d-2ba5-407a-c54f-d90584366de2"
      },
      "source": [
        "print(chunks_lst[1])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Which type vaccine Canada\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuSxbBfu2NqN"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
        "from collections import defaultdict\n",
        "from sklearn.svm import LinearSVC, LinearSVR\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXXhAL13Pj6z"
      },
      "source": [
        "ner_vectorizer = CountVectorizer(ngram_range=(1,1),min_df=2)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCLt2L-APsVT"
      },
      "source": [
        "NER_vec = ner_vectorizer.fit_transform(chunks_lst)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpia1RodRQTc",
        "outputId": "64b9de8b-2641-4bc2-f07b-51ce27177ca0"
      },
      "source": [
        "NER_vec.shape"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20660, 3507)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AEwP7YhQeUG"
      },
      "source": [
        "vectorizer = CountVectorizer(ngram_range=(1,2),min_df=2)\n",
        "train_texts = train_df.Comment.values\n",
        "\n",
        "X_train = vectorizer.fit_transform(train_texts)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GLd4CHcRTS1",
        "outputId": "ec3df721-43e1-4d27-e5f7-1e5ea3fca16a"
      },
      "source": [
        "from scipy.sparse import coo_matrix, hstack\n",
        "X_train = hstack([X_train, NER_vec])\n",
        "\n",
        "X_train.shape"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20660, 29718)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-NwOVKrR2hE"
      },
      "source": [
        "def prepare_for_classification(train,dev,max_n=2):\n",
        "  '''convert lists of reviews train and dev to spare feature matrices X_train and X_test,\n",
        "  and lists of polarity classifications train_class and dev_class'''\n",
        "  vectorizer = CountVectorizer(ngram_range=(1,max_n),min_df=2)\n",
        "  ner_vectorizer = CountVectorizer(ngram_range=(1,1),min_df=2)\n",
        "  ner_train = ner_vectorizer.fit_transform(chunks_lst)\n",
        "  train_texts = train.Comment.values\n",
        "  train_class = train.Tags.values\n",
        "  dev_texts = dev.Comment.values\n",
        "  dev_class = dev.Tags.values\n",
        "  X_train = vectorizer.fit_transform(train_texts)\n",
        "  X_train = hstack([X_train, ner_train])\n",
        "  X_dev = vectorizer.transform(dev_texts)\n",
        "  ner_dev = ner_vectorizer.transform(dev_texts)\n",
        "  X_dev = hstack([X_dev, ner_dev])\n",
        "  return X_train,train_class, X_dev,dev_class"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JXAyZHTR71v",
        "outputId": "5cc5631b-0482-4ddf-fb11-a0de89e6657f"
      },
      "source": [
        "def evluate(train, test, n = 2):\n",
        "    \"\"\"Calculate the the kendalltau score from given train test data set, and n grams\"\"\"\n",
        "    \n",
        "    X_train,train_class, X_test,test_class = prepare_for_classification(train,test,max_n=n)\n",
        "    clf = LinearSVC()\n",
        "    clf.fit(X_train,train_class)\n",
        "    \n",
        "    fscore = f1_score(test_class, clf.predict(X_test), average='macro')\n",
        "    acc = accuracy_score(test_class, clf.predict(X_test))\n",
        "    \n",
        "    print(f\"{n}-gram: The shape of training set is {X_train.shape}, the fscore is {fscore}, the accuracy is {acc}\")\n",
        "\n",
        "    return fscore\n",
        "    \n",
        " \n",
        "print(\"Vaccine:\")\n",
        "for n in range(1, 6):\n",
        "    evluate(train_df,dev_df, n)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vaccine:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1-gram: The shape of training set is (20660, 8462), the fscore is 0.6199512593917744, the accuracy is 0.7473378509196515\n",
            "2-gram: The shape of training set is (20660, 29718), the fscore is 0.6617908517897326, the accuracy is 0.7864472410454986\n",
            "3-gram: The shape of training set is (20660, 49829), the fscore is 0.6664901825064218, the accuracy is 0.7930300096805422\n",
            "4-gram: The shape of training set is (20660, 61759), the fscore is 0.670085611169692, the accuracy is 0.7961277831558567\n",
            "5-gram: The shape of training set is (20660, 67761), the fscore is 0.6702186034348959, the accuracy is 0.7955469506292352\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}