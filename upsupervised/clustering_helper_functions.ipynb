{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This file includes functions to:\n",
    "   - Detect potential clusterings that may have new tags\n",
    "   - Generate candidate tags for these clusterings\n",
    "   - Linguistic feature embedding(dependecy parsing)\n",
    "\n",
    "(They work for English and French.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the following packages. You can run python -m spacy download fr_core_news_sm to downlowad if you do not have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_fr = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detect_new_tag_clustering(cluster,threshold=0.6):\n",
    "    \"\"\"\n",
    "    This function determine whether the input cluster needs a new tag or not. \n",
    "    If percentage of the dominant tag is over shreshold hundredths, this cluster does not need a new tag.\n",
    "    If percentage of the dominant tag is below shreshold hundredths, this cluster needs a new tag.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "        cluster: a list of tags in the cluster\n",
    "    \n",
    "    Return:\n",
    "    -----------\n",
    "        boolean: yes-this cluster needs a new tag;no-this cluster does not need a new tag\n",
    "    \"\"\"\n",
    "    counter_dict = defaultdict(int)\n",
    "    for tag in cluster:\n",
    "        counter_dict[tag]+=1\n",
    "    total = sum(list(counter_dict.values()))\n",
    "    tag_count_pairs = sorted(list(counter_dict.items()),key = lambda x: x[1],reverse=True)\n",
    "    if tag_count_pairs[0][1]/total>threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster1 = [\"Tag1\",\"Tag2\",\"Tag1\",\"Tag2\",\"Tag1\",\"Tag1\",\"Tag1\",\"Tag1\",\"Tag1\"]\n",
    "detect_new_tag_clustering(cluster1)\n",
    "cluster2 = [\"Tag5\",\"Tag2\",\"Tag1\",\"Tag2\",\"Tag1\",\"Tag7\",\"Tag1\",\"Tag5\",\"Tag1\"]\n",
    "detect_new_tag_clustering(cluster2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tag_candidates(comments, approach=\"NER\", langauge=\"English\", top=10):\n",
    "    \"\"\"\n",
    "    This function takes a list of comments/indices and generate a list of potential tags.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "        comments: list of str\n",
    "        approach: define the approach to generate new tags. The default one is \"NER\". You can also use \"unigram\", \"bigram\". \n",
    "        langauge: English or French\n",
    "        top: return top 10(defalut) common entities\n",
    "    Return:\n",
    "    -----------\n",
    "        candidates: list of str\n",
    "    \"\"\"\n",
    "\n",
    "    if langauge==\"English\":\n",
    "        nlp = nlp_en\n",
    "    elif langauge==\"French\":\n",
    "        nlp = nlp_fr\n",
    "    \n",
    "    for comment in comments:\n",
    "        doc = nlp(comment)\n",
    "        if approach==\"NER\":\n",
    "            entity_counter_dict = defaultdict(int)\n",
    "            for ent in doc.ents:\n",
    "                entity_counter_dict[ent.text] += 1\n",
    "            entity_count_pairs = sorted(list(entity_counter_dict.items()),key = lambda x: x[1],reverse=True)\n",
    "            return [ent for (ent, count) in entity_count_pairs[:top]]\n",
    "        if approach==\"bigram\" or approach==\"trigram\":\n",
    "            n_gram_counter_dict = defaultdict(int)\n",
    "            tokens = [token.lemma_ for token in doc]\n",
    "            n=2 if approach==\"bigram\" else 3\n",
    "            n_grams = ngrams(tokens, n)\n",
    "            for n_gram in n_grams:\n",
    "                n_gram_counter_dict[n_gram] += 1\n",
    "            n_gram_count_pairs = sorted(list(n_gram_counter_dict.items()),key = lambda x: x[1],reverse=True)\n",
    "            return [n_gram for (n_gram, count) in n_gram_count_pairs[:top]]\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('how', 'to', 'enter'),\n",
       " ('to', 'enter', 'Canada'),\n",
       " ('enter', 'Canada', 'from'),\n",
       " ('Canada', 'from', 'U.S'),\n",
       " ('from', 'U.S', '?')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = [\"How to enter Canada from U.S?\", \"Where is Canada?\"]\n",
    "generate_tag_candidates(comments, approach=\"trigram\", langauge=\"English\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cited from [here](https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open(\"../data/glove.6B/glove.6B.100d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    \n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50.5,  1. , 26. ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = [[1,2,2],[100,0,50]]\n",
    "np.array(v).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_embedding(pred_list, dim=100):\n",
    "    \"\"\"\n",
    "    This function maps a list of pred to a 1x100 200 or 300 vector(the default one is 100d)\n",
    "    Parameters:\n",
    "    ------------\n",
    "        pred_list: list of str\n",
    "    Return:\n",
    "    ------------\n",
    "        emb: 1xdim array\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for pred in pred_list:\n",
    "        if pred in embeddings_dict:\n",
    "            vectors.append(embeddings_dict[pred])\n",
    "        else:\n",
    "            vectors.append([0]*dim)\n",
    "    return np.array(vectors).mean(axis=0) # get mean     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11049001,  0.37868   ,  0.13138498, -0.34656549, -0.31117502,\n",
       "        0.10618401, -0.35618   ,  0.26565   ,  0.1083475 ,  0.0336    ,\n",
       "       -0.10457   , -0.02349998,  0.086239  ,  0.31344   ,  0.30633998,\n",
       "       -0.24361001,  0.46379   ,  0.422775  , -0.49794   ,  0.307405  ,\n",
       "        0.1629845 ,  0.07167001,  0.181355  , -0.15389001, -0.0874075 ,\n",
       "        0.024791  , -0.22953   , -0.63475   ,  0.23915249, -0.08699501,\n",
       "        0.012312  ,  0.960665  , -0.49604   ,  0.163051  ,  0.43383002,\n",
       "        0.392805  , -0.34657502,  0.130058  ,  0.205599  ,  0.1603395 ,\n",
       "       -0.262295  , -0.37798   , -0.27833   , -0.67448497, -0.34548002,\n",
       "        0.052679  , -0.206057  , -0.51545   ,  0.133735  , -1.0046    ,\n",
       "       -0.06104501,  0.410275  , -0.21164551,  1.121715  ,  0.0728007 ,\n",
       "       -2.2982502 ,  0.217875  , -0.15031   ,  1.9992499 , -0.05432   ,\n",
       "       -0.066075  ,  0.822565  , -0.261895  ,  0.2889545 ,  0.98931503,\n",
       "       -0.11211199,  0.593485  ,  0.068685  , -0.01613   , -0.51545   ,\n",
       "       -0.26087725, -0.465345  ,  0.09254099, -0.37366   , -0.0535795 ,\n",
       "        0.06817999, -0.2786585 , -0.06604   , -0.66503   ,  0.01195   ,\n",
       "        0.868095  , -0.453663  , -0.786165  , -0.06936999, -1.8004    ,\n",
       "        0.24989001,  0.60390496,  0.059156  , -0.3673085 , -0.2571335 ,\n",
       "       -0.1760925 , -0.12830049,  0.0394245 , -0.576795  , -0.39258498,\n",
       "       -0.433165  , -0.10494499, -0.10115001,  0.732225  ,  0.26017648],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_embedding([\"use\",\"get\"], dim=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
